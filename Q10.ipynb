{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 10: \n",
    "\n",
    "For each time period described in Question 6, perform the same grid search above for GradientBoostingRegressor (with corresponding time window length). Does the cross-validation test error change? Are the best parameter set you find in each period agree with those you found above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_tags = ['#gohawks','#gopatriots','#nfl','#patriots','#sb49','#superbowl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_object(data, fileName):\n",
    "    with open('pynb_data/'+fileName + \".pickle\", 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_object(fileName):\n",
    "    try:\n",
    "        with open('pynb_data/'+fileName + \".pickle\", 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            return data\n",
    "    except IOError:\n",
    "        print(\"Could not read file: \" + fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def getMinAndMaxTs(tag):\n",
    "    filename = 'data/tweets_'+tag+'.txt'\n",
    "    max_ts = 0\n",
    "    min_ts = 1552522378\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            json_object = json.loads(line)\n",
    "            timestamp = json_object['citation_date']\n",
    "            if(timestamp < min_ts):                \n",
    "                min_ts = timestamp\n",
    "            \n",
    "            if(timestamp > max_ts):\n",
    "                max_ts = timestamp\n",
    "                \n",
    "    return [min_ts,max_ts]\n",
    "\n",
    "tagsToMinTs = {}\n",
    "tagsToMaxTs = {}\n",
    "for tag in hash_tags:\n",
    "    ts_list = getMinAndMaxTs(tag)\n",
    "    tagsToMinTs[tag] = (ts_list[0])\n",
    "    tagsToMaxTs[tag] = (ts_list[1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "def getLocalHour(timestamp):\n",
    "    d = datetime.datetime.fromtimestamp(timestamp)\n",
    "    pst = pytz.timezone('America/Los_Angeles')\n",
    "    d = pst.localize(d)\n",
    "    return d.hour\n",
    "\n",
    "def getWindowNumber(start_ts, curr_ts, window):\n",
    "    elapsed = (curr_ts - start_ts)/(window*1.0)\n",
    "    windowNum = math.ceil(elapsed)\n",
    "    return windowNum    \n",
    "\n",
    "def getFeatures(start_ts,end_ts,window):\n",
    "    windowToTweets = {}\n",
    "    windowToRetweets = {}\n",
    "    windowToFollowerCount = {}\n",
    "    windowToMaxFollowers = {}\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for tag in hash_tags:\n",
    "        filename = 'data/tweets_'+tag+'.txt'\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                json_object = json.loads(line)\n",
    "                timestamp = json_object['citation_date']\n",
    "            \n",
    "                if timestamp < start_ts or timestamp > end_ts:                            \n",
    "                    continue\n",
    "                \n",
    "                key = getWindowNumber(start_ts,timestamp,window)\n",
    "    #             print(key)\n",
    "                if key not in windowToTweets.keys():\n",
    "                    windowToTweets[key]=0\n",
    "                windowToTweets[key]+=1\n",
    "            \n",
    "                retweetCount = json_object['metrics']['citations']['total']        \n",
    "            \n",
    "                if key not in windowToRetweets.keys():\n",
    "                    windowToRetweets[key]=0\n",
    "                windowToRetweets[key]+=retweetCount\n",
    "        \n",
    "                followerCount = json_object['author']['followers']\n",
    "                if key not in windowToFollowerCount.keys():\n",
    "                    windowToFollowerCount[key]=0\n",
    "                windowToFollowerCount[key]+=followerCount\n",
    "        \n",
    "                if key not in windowToMaxFollowers.keys():\n",
    "                    windowToMaxFollowers[key]=0\n",
    "                windowToMaxFollowers[key] = max(windowToMaxFollowers[key],followerCount)            \n",
    "            \n",
    "    for period in range(start_ts,end_ts,window):\n",
    "        key = getWindowNumber(start_ts,period,window)\n",
    "        tweetCount = windowToTweets.get(key, 0)\n",
    "        retweetCount = windowToRetweets.get(key,0)\n",
    "        followerCount = windowToFollowerCount.get(key,0)\n",
    "        maxFollowers = windowToMaxFollowers.get(key,0)\n",
    "\n",
    "        h = getLocalHour(key)\n",
    "            \n",
    "        feature = [tweetCount, retweetCount, followerCount, maxFollowers, h]\n",
    "        features.append(feature)\n",
    "                \n",
    "        nextKey = getWindowNumber(start_ts, period + window, window)\n",
    "        labels.append(windowToTweets.get(nextKey,0))\n",
    "                \n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440\n",
      "[111, 89, 110, 100, 137, 169, 215, 353, 569, 533, 530, 544, 525, 628, 611, 675, 260, 256, 233, 342, 402, 334, 258, 119, 88, 275, 155, 173, 336, 160, 291, 479, 632, 563, 304, 528, 651, 534, 626, 614, 376, 398, 496, 387, 243, 258, 203, 65, 145, 34, 20, 42, 180, 115, 185, 687, 885, 954, 704, 981, 1032, 825, 884, 799, 474, 799, 724, 473, 459, 337, 334, 236, 125, 103, 82, 133, 218, 93, 192, 309, 500, 612, 430, 710, 783, 687, 686, 670, 428, 712, 697, 614, 619, 542, 582, 417, 434, 347, 139, 21, 312, 544, 1141, 1839, 2459, 3087, 2493, 6380, 10260, 6702, 7271, 39421, 14572, 8757, 22584, 18564, 4389, 2640, 1997, 1413, 914, 709, 721, 666, 1510, 1123, 1475, 2183, 1964, 1975, 1158, 1718, 1546, 1413, 1080, 1221, 1293, 1270, 7962, 1361, 654, 658, 443, 409, 287, 204, 219, 221, 336, 282, 434, 690, 840, 957, 511, 775, 46, 85, 705, 786, 524, 817, 811, 670, 3310, 2254, 995, 609, 329, 261, 343, 667, 975, 739, 838, 928, 870, 858, 844, 843, 842, 767, 783, 1087, 1101, 1263, 931, 966, 814, 793, 416, 409, 212, 138, 205, 180, 322, 297, 951, 934, 1476, 1390, 862, 577, 548, 131, 108, 91, 680, 1585, 1544, 1044, 987, 749, 590, 377, 310, 223, 225, 320, 422, 562, 939, 1469, 1497, 954, 1333, 1496, 1777, 1230, 1152, 691, 1147, 978, 962, 728, 606, 504, 384, 342, 239, 186, 225, 234, 161, 396, 972, 630, 749, 679, 958, 1449, 5358, 1470, 1153, 825, 811, 717, 792, 717, 688, 579, 437, 369, 222, 181, 163, 201, 226, 292, 508, 665, 1274, 1225, 1579, 1432, 1196, 1173, 1166, 1014, 923, 1204, 1122, 907, 931, 651, 537, 405, 296, 234, 328, 343, 380, 530, 784, 1013, 1500, 1163, 1158, 1356, 804, 1173, 1723, 1187, 1113, 4268, 1784, 1442, 1420, 952, 693, 536, 288, 302, 315, 419, 560, 704, 909, 1246, 1412, 2328, 2168, 2896, 2197, 1689, 1707, 1584, 1216, 1242, 1460, 1127, 1244, 931, 668, 513, 351, 256, 265, 389, 542, 422, 582, 1216, 1610, 1878, 1831, 1716, 1600, 1528, 1700, 1784, 1566, 1464, 1207, 1112, 297, 48, 29, 37, 722, 684, 744, 839, 940, 1347, 1883, 2610, 3033, 3382, 3377, 2423, 2054, 2281, 2340, 2240, 2978, 2536, 1632, 2496, 1032, 536, 1411, 1025, 982, 789, 793, 1345, 1047, 386, 1032, 3902, 4719, 3268, 4883, 6689, 6044, 4686, 4600, 2654, 1399, 400, 911, 506, 554, 531, 394, 293, 239, 293, 249, 282, 376, 525, 708, 916, 1111, 1262, 1328, 1185, 1620, 1432, 1573, 1470, 3976, 4932, 4864, 2368, 3730, 3318, 3031, 2143, 2012, 2173, 2267, 2505, 3267, 5793, 9022, 12048]\n",
      "-5129879.083650498\n",
      "{'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# http://aplunket.com/random-forest-regressor/\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#tp1\n",
    "tp1_window_size = 3600 # 1 hour window size\n",
    "tp1_start_ts = tp1_window_size * math.floor(tagsToMinTs[tag]/(tp1_window_size*1.0))\n",
    "tp1_end_ts = 1422806400\n",
    "# features,labels = getFeatures(tp1_start_ts,tp1_end_ts,tp1_window_size)\n",
    "print(len(features))\n",
    "print(labels)\n",
    "# save_object(features, \"q7_tp1_features_{}\".format(tag))\n",
    "# save_object(labels, \"q7_tp1_labels_{}\".format(tag))\n",
    "\n",
    "features = load_object(\"q7_tp1_features\")\n",
    "labels = load_object(\"q7_tp1_labels\")\n",
    "    \n",
    "\n",
    "\n",
    "param_grid = {\n",
    "  'max_depth': [10, 20, 40, 60, 80, 100, 200, None],\n",
    "  'max_features': ['auto', 'sqrt'],\n",
    "  'min_samples_leaf': [1, 2, 4],\n",
    "  'min_samples_split': [2, 5, 10],\n",
    "  'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}\n",
    "\n",
    "model = GradientBoostingRegressor(random_state=0)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv = KFold(5, shuffle=True), scoring='neg_mean_squared_error')\n",
    "grid.fit(features, labels)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440\n",
      "[111, 89, 110, 100, 137, 169, 215, 353, 569, 533, 530, 544, 525, 628, 611, 675, 260, 256, 233, 342, 402, 334, 258, 119, 88, 275, 155, 173, 336, 160, 291, 479, 632, 563, 304, 528, 651, 534, 626, 614, 376, 398, 496, 387, 243, 258, 203, 65, 145, 34, 20, 42, 180, 115, 185, 687, 885, 954, 704, 981, 1032, 825, 884, 799, 474, 799, 724, 473, 459, 337, 334, 236, 125, 103, 82, 133, 218, 93, 192, 309, 500, 612, 430, 710, 783, 687, 686, 670, 428, 712, 697, 614, 619, 542, 582, 417, 434, 347, 139, 21, 312, 544, 1141, 1839, 2459, 3087, 2493, 6380, 10260, 6702, 7271, 39421, 14572, 8757, 22584, 18564, 4389, 2640, 1997, 1413, 914, 709, 721, 666, 1510, 1123, 1475, 2183, 1964, 1975, 1158, 1718, 1546, 1413, 1080, 1221, 1293, 1270, 7962, 1361, 654, 658, 443, 409, 287, 204, 219, 221, 336, 282, 434, 690, 840, 957, 511, 775, 46, 85, 705, 786, 524, 817, 811, 670, 3310, 2254, 995, 609, 329, 261, 343, 667, 975, 739, 838, 928, 870, 858, 844, 843, 842, 767, 783, 1087, 1101, 1263, 931, 966, 814, 793, 416, 409, 212, 138, 205, 180, 322, 297, 951, 934, 1476, 1390, 862, 577, 548, 131, 108, 91, 680, 1585, 1544, 1044, 987, 749, 590, 377, 310, 223, 225, 320, 422, 562, 939, 1469, 1497, 954, 1333, 1496, 1777, 1230, 1152, 691, 1147, 978, 962, 728, 606, 504, 384, 342, 239, 186, 225, 234, 161, 396, 972, 630, 749, 679, 958, 1449, 5358, 1470, 1153, 825, 811, 717, 792, 717, 688, 579, 437, 369, 222, 181, 163, 201, 226, 292, 508, 665, 1274, 1225, 1579, 1432, 1196, 1173, 1166, 1014, 923, 1204, 1122, 907, 931, 651, 537, 405, 296, 234, 328, 343, 380, 530, 784, 1013, 1500, 1163, 1158, 1356, 804, 1173, 1723, 1187, 1113, 4268, 1784, 1442, 1420, 952, 693, 536, 288, 302, 315, 419, 560, 704, 909, 1246, 1412, 2328, 2168, 2896, 2197, 1689, 1707, 1584, 1216, 1242, 1460, 1127, 1244, 931, 668, 513, 351, 256, 265, 389, 542, 422, 582, 1216, 1610, 1878, 1831, 1716, 1600, 1528, 1700, 1784, 1566, 1464, 1207, 1112, 297, 48, 29, 37, 722, 684, 744, 839, 940, 1347, 1883, 2610, 3033, 3382, 3377, 2423, 2054, 2281, 2340, 2240, 2978, 2536, 1632, 2496, 1032, 536, 1411, 1025, 982, 789, 793, 1345, 1047, 386, 1032, 3902, 4719, 3268, 4883, 6689, 6044, 4686, 4600, 2654, 1399, 400, 911, 506, 554, 531, 394, 293, 239, 293, 249, 282, 376, 525, 708, 916, 1111, 1262, 1328, 1185, 1620, 1432, 1573, 1470, 3976, 4932, 4864, 2368, 3730, 3318, 3031, 2143, 2012, 2173, 2267, 2505, 3267, 5793, 9022, 12048]\n",
      "-28896513.83610084\n",
      "{'max_depth': 40, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabketandoshi/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# http://aplunket.com/random-forest-regressor/\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#tp1\n",
    "tp1_window_size = 3600 # 1 hour window size\n",
    "tp1_start_ts = tp1_window_size * math.floor(tagsToMinTs[tag]/(tp1_window_size*1.0))\n",
    "tp1_end_ts = 1422806400\n",
    "# features,labels = getFeatures(tp1_start_ts,tp1_end_ts,tp1_window_size)\n",
    "print(len(features))\n",
    "print(labels)\n",
    "# save_object(features, \"q7_tp1_features_{}\".format(tag))\n",
    "# save_object(labels, \"q7_tp1_labels_{}\".format(tag))\n",
    "\n",
    "features = load_object(\"q7_tp2_features\")\n",
    "labels = load_object(\"q7_tp2_labels\")\n",
    "    \n",
    "\n",
    "\n",
    "param_grid = {\n",
    "  'max_depth': [10, 20, 40, 60, 80, 100, 200, None],\n",
    "  'max_features': ['auto', 'sqrt'],\n",
    "  'min_samples_leaf': [1, 2, 4],\n",
    "  'min_samples_split': [2, 5, 10],\n",
    "  'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}\n",
    "\n",
    "model = GradientBoostingRegressor(random_state=0)\n",
    "tp2grid = GridSearchCV(estimator=model, param_grid=param_grid, cv = KFold(5, shuffle=True), scoring='neg_mean_squared_error')\n",
    "tp2grid.fit(features, labels)\n",
    "\n",
    "print(tp2grid.best_score_)\n",
    "print(tp2grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "[1300, 1343, 1226, 1302, 1224, 319, 245, 231, 230, 237, 284, 266, 470, 281, 332, 400, 1401, 1493, 1709, 1494, 1593, 1463, 1107, 1716, 1861, 1611, 1544, 1560, 1770, 1676, 1820, 1569, 1697, 8499, 9648, 9991, 5736, 9791, 14318, 15471, 15151, 15241, 13539, 13771, 13805, 15622, 14181, 14508, 13067, 13880, 12876, 12221, 11966, 12615, 11745, 11469, 11083, 11119, 10466, 11276, 10371, 11089, 10611, 10115, 9768, 9986, 9730, 8193, 7300, 7410, 11060, 9837, 9391, 9201, 9217, 8763, 9215, 8532, 9360, 10176, 10267, 10557, 10779, 12747, 20744, 19360, 27623, 25137, 34867, 32175, 34416, 23369, 22546, 20954, 27243, 25419, 23214, 24047, 36027, 25872, 23175, 26587, 30303, 28063, 22709, 26830, 23483, 27450, 34990, 25681, 35025, 45904, 45773, 37186, 23920, 23983, 19550, 17554, 24798, 22708, 13866, 14376, 12927, 12568, 14721, 19292, 17790, 14340, 12789, 28000, 16210, 21143, 35905, 41702, 24998, 17300, 13897, 12540, 9347, 8027, 6915, 5890, 3177, 610]\n",
      "-2530331.111027386\n",
      "{'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# http://aplunket.com/random-forest-regressor/\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#tp1\n",
    "tp1_window_size = 3600 # 1 hour window size\n",
    "tp1_start_ts = tp1_window_size * math.floor(tagsToMinTs[tag]/(tp1_window_size*1.0))\n",
    "tp1_end_ts = 1422806400\n",
    "# features,labels = getFeatures(tp1_start_ts,tp1_end_ts,tp1_window_size)\n",
    "print(len(features))\n",
    "print(labels)\n",
    "# save_object(features, \"q7_tp1_features_{}\".format(tag))\n",
    "# save_object(labels, \"q7_tp1_labels_{}\".format(tag))\n",
    "\n",
    "features = load_object(\"q7_tp3_features\")\n",
    "labels = load_object(\"q7_tp3_labels\")\n",
    "    \n",
    "\n",
    "\n",
    "param_grid = {\n",
    "  'max_depth': [10, 20, 40, 60, 80, 100, 200, None],\n",
    "  'max_features': ['auto', 'sqrt'],\n",
    "  'min_samples_leaf': [1, 2, 4],\n",
    "  'min_samples_split': [2, 5, 10],\n",
    "  'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}\n",
    "\n",
    "model = GradientBoostingRegressor(random_state=0)\n",
    "tp3grid = GridSearchCV(estimator=model, param_grid=param_grid, cv = KFold(5, shuffle=True), scoring='neg_mean_squared_error')\n",
    "tp3grid.fit(features, labels)\n",
    "\n",
    "print(tp3grid.best_score_)\n",
    "print(tp3grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
