{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 14: \n",
    "Report the model you use. For each test file, provide your predictions on the number of tweets in the next time window.\n",
    "Note: Test data should not be used as a source for training. You are not bounded to only linear models. You can find your best model through cross validation of your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp1_files = ['sample0_period1.txt','sample1_period1.txt','sample2_period1.txt']\n",
    "tp2_files = ['sample0_period2.txt','sample1_period2.txt','sample2_period2.txt']\n",
    "tp3_files = ['sample0_period3.txt','sample1_period3.txt','sample2_period3.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_object(data, fileName):\n",
    "    with open('pynb_data/'+fileName + \".pickle\", 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_object(fileName):\n",
    "    try:\n",
    "        with open('pynb_data/'+fileName + \".pickle\", 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            return data\n",
    "    except IOError:\n",
    "        print(\"Could not read file: \" + fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_tags = ['#gohawks','#gopatriots','#nfl','#patriots','#sb49','#superbowl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def getMinAndMaxTs(tag,filename=None):\n",
    "    if filename is None:\n",
    "        filename = 'data/tweets_'+tag+'.txt'\n",
    "    max_ts = 0\n",
    "    min_ts = 1552522378\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            json_object = json.loads(line)\n",
    "            timestamp = json_object['citation_date']\n",
    "            if(timestamp < min_ts):                \n",
    "                min_ts = timestamp\n",
    "            \n",
    "            if(timestamp > max_ts):\n",
    "                max_ts = timestamp\n",
    "                \n",
    "    return [min_ts,max_ts]\n",
    "\n",
    "tagsToMinTs = {}\n",
    "tagsToMaxTs = {}\n",
    "globalMinTs = 1552522378\n",
    "globalMaxTs = 0\n",
    "for tag in hash_tags:\n",
    "    ts_list = getMinAndMaxTs(tag)\n",
    "    \n",
    "    if(ts_list[0]<globalMinTs):\n",
    "        globalMinTs = ts_list[0]\n",
    "    \n",
    "    if(ts_list[1]>globalMaxTs):\n",
    "        globalMaxTs = ts_list[1]\n",
    "        \n",
    "    tagsToMinTs[tag] = (ts_list[0])\n",
    "    tagsToMaxTs[tag] = (ts_list[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filesToMinTs = {}\n",
    "filesToMaxTs = {}\n",
    "\n",
    "for file in tp1_files:\n",
    "    ts_list = getMinAndMaxTs(None,'test_data/'+file)\n",
    "    filesToMinTs[file] = (ts_list[0])\n",
    "    filesToMaxTs[file] = (ts_list[1])    \n",
    "    \n",
    "for file in tp2_files:\n",
    "    ts_list = getMinAndMaxTs(None,'test_data/'+file)\n",
    "    filesToMinTs[file] = (ts_list[0])\n",
    "    filesToMaxTs[file] = (ts_list[1])    \n",
    "    \n",
    "for file in tp3_files:\n",
    "    ts_list = getMinAndMaxTs(None,'test_data/'+file)\n",
    "    filesToMinTs[file] = (ts_list[0])\n",
    "    filesToMaxTs[file] = (ts_list[1])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "def getLocalHour(timestamp):\n",
    "    d = datetime.datetime.fromtimestamp(timestamp)\n",
    "    pst = pytz.timezone('America/Los_Angeles')\n",
    "    d = pst.localize(d)\n",
    "    return d.hour\n",
    "\n",
    "def getWindowNumber(start_ts, curr_ts, window):\n",
    "    elapsed = (curr_ts - start_ts)/(window*1.0)\n",
    "    windowNum = math.ceil(elapsed)\n",
    "    return windowNum    \n",
    "\n",
    "def getFeatures(start_ts,end_ts,window):\n",
    "    windowToTweets = {}\n",
    "    windowToRetweets = {}\n",
    "    windowToFollowerCount = {}\n",
    "    windowToMaxFollowers = {}\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for tag in hash_tags:\n",
    "        filename = 'data/tweets_'+tag+'.txt'\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                json_object = json.loads(line)\n",
    "                timestamp = json_object['citation_date']\n",
    "            \n",
    "                if timestamp < start_ts or timestamp > end_ts:                            \n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                key = getWindowNumber(start_ts,timestamp,window)\n",
    "    #             print(key)\n",
    "                if key not in windowToTweets.keys():\n",
    "                    windowToTweets[key]=0\n",
    "                windowToTweets[key]+=1\n",
    "            \n",
    "                retweetCount = json_object['metrics']['citations']['total']        \n",
    "            \n",
    "                if key not in windowToRetweets.keys():\n",
    "                    windowToRetweets[key]=0\n",
    "                windowToRetweets[key]+=retweetCount\n",
    "        \n",
    "                followerCount = json_object['author']['followers']\n",
    "                if key not in windowToFollowerCount.keys():\n",
    "                    windowToFollowerCount[key]=0\n",
    "                windowToFollowerCount[key]+=followerCount\n",
    "        \n",
    "                if key not in windowToMaxFollowers.keys():\n",
    "                    windowToMaxFollowers[key]=0\n",
    "                windowToMaxFollowers[key] = max(windowToMaxFollowers[key],followerCount)            \n",
    "            \n",
    "    for period in range(start_ts,end_ts,window):\n",
    "        key = getWindowNumber(start_ts,period,window)\n",
    "        tweetCount = windowToTweets.get(key, 0)\n",
    "        retweetCount = windowToRetweets.get(key,0)\n",
    "        followerCount = windowToFollowerCount.get(key,0)\n",
    "        maxFollowers = windowToMaxFollowers.get(key,0)\n",
    "\n",
    "        h = getLocalHour(period)\n",
    "            \n",
    "        feature = [tweetCount, retweetCount, followerCount, maxFollowers, h]\n",
    "        features.append(feature)\n",
    "                \n",
    "        nextKey = getWindowNumber(start_ts, period + window, window)\n",
    "        labels.append(windowToTweets.get(nextKey,0))\n",
    "                \n",
    "    return features,labels\n",
    "\n",
    "def getFeaturesFromFile(filename,start_ts,end_ts,window):\n",
    "    windowToTweets = {}\n",
    "    windowToRetweets = {}\n",
    "    windowToFollowerCount = {}\n",
    "    windowToMaxFollowers = {}\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        for line in f:            \n",
    "            json_object = json.loads(line)\n",
    "            timestamp = json_object['citation_date']\n",
    "            \n",
    "            if timestamp < start_ts or timestamp > end_ts:                            \n",
    "                continue\n",
    "                \n",
    "            key = getWindowNumber(start_ts,timestamp,window)\n",
    "            if key not in windowToTweets.keys():\n",
    "                windowToTweets[key]=0\n",
    "            windowToTweets[key]+=1\n",
    "            \n",
    "            retweetCount = json_object['metrics']['citations']['total']        \n",
    "            \n",
    "            if key not in windowToRetweets.keys():\n",
    "                windowToRetweets[key]=0\n",
    "            windowToRetweets[key]+=retweetCount\n",
    "        \n",
    "            followerCount = json_object['author']['followers']\n",
    "            if key not in windowToFollowerCount.keys():\n",
    "                windowToFollowerCount[key]=0\n",
    "            windowToFollowerCount[key]+=followerCount\n",
    "        \n",
    "            if key not in windowToMaxFollowers.keys():\n",
    "                windowToMaxFollowers[key]=0\n",
    "            windowToMaxFollowers[key] = max(windowToMaxFollowers[key],followerCount)            \n",
    "    \n",
    "#     print(len([i for i in range(start_ts,end_ts,window)]\n",
    "            \n",
    "    for period in range(start_ts,end_ts,window):\n",
    "        key = getWindowNumber(start_ts,period,window)\n",
    "        tweetCount = windowToTweets.get(key, 0)\n",
    "        retweetCount = windowToRetweets.get(key,0)\n",
    "        followerCount = windowToFollowerCount.get(key,0)\n",
    "        maxFollowers = windowToMaxFollowers.get(key,0)\n",
    "\n",
    "        h = getLocalHour(period)\n",
    "            \n",
    "        feature = [tweetCount, retweetCount, followerCount, maxFollowers, h]\n",
    "        if tweetCount > 0:\n",
    "            features.append(feature)\n",
    "                \n",
    "            nextKey = getWindowNumber(start_ts, period + window, window)\n",
    "            labels.append(windowToTweets.get(nextKey,0))\n",
    "                \n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeperiod 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building training set for Timeperiod 1(same as q7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Building feature vectors for training set for time period 1\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "#tp1\n",
    "tp1_window_size = 3600 # 1 hour window size\n",
    "tp1_start_ts = tp1_window_size * math.floor(globalMinTs/(tp1_window_size*1.0))\n",
    "tp1_end_ts = 1422806400\n",
    "# tp1_train_features,tp1_train_labels = getFeatures(tp1_start_ts,tp1_end_ts,tp1_window_size)\n",
    "# save_object(tp1_train_features, \"q14_tp1_train_features\")\n",
    "# save_object(tp1_train_labels, \"q14_tp1_train_labels\")\n",
    "tp1_train_features = load_object(\"q14_tp1_train_features\")\n",
    "tp1_train_labels = load_object(\"q14_tp1_train_labels\")\n",
    "print(\"Finished Building feature vectors for training set for time period 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building test set for Timeperiod 1(sample 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1422709237\n",
      "1422709200\n",
      "[[52, 109, 424498.0, 168371.0, 6], [79, 761, 2975692.0, 2034387.0, 7], [94, 226, 860594.0, 328882.0, 8], [101, 258, 2349147.0, 368626.0, 9], [122, 483, 1369748.0, 291130.0, 10], [120, 322, 9022480.0, 5883161.0, 11]]\n",
      "Finished Building test feature vectors for time period 1 sample 0\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "file = 'sample0_period1.txt'\n",
    "\n",
    "print(filesToMinTs[file])\n",
    "#tp1\n",
    "tp1_window_size = 3600 # 1 hour window size\n",
    "tp1_start_ts = tp1_window_size * math.floor(filesToMinTs[file]/(tp1_window_size*1.0))\n",
    "print(tp1_start_ts)\n",
    "tp1_end_ts = 1422806400\n",
    "tp1_test_p1_s0_features,tp1_test_p1_s0_labels = getFeaturesFromFile('test_data/'+file,tp1_start_ts,tp1_end_ts,tp1_window_size)\n",
    "print(tp1_test_p1_s0_features)\n",
    "save_object(tp1_test_p1_s0_features, \"q14_tp1_test_p1_s0_features\")\n",
    "save_object(tp1_test_p1_s0_labels, \"q14_tp1_test_p1_s0_labels\")\n",
    "# tp1_test_p1_s0_features = load_object(\"q14_tp1_test_p1_s0_features\")\n",
    "# tp1_test_p1_s0_labels = load_object(\"q14_tp1_test_p1_s0_labels\")\n",
    "print(\"Finished Building test feature vectors for time period 1 sample 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building test set for Timeperiod 1(sample 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Finished Building test feature vectors for time period 1 sample 1\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "file = 'sample1_period1.txt'\n",
    "\n",
    "#tp1\n",
    "tp1_window_size = 3600 # 1 hour window size\n",
    "tp1_start_ts = tp1_window_size * math.floor(filesToMinTs[file]/(tp1_window_size*1.0))\n",
    "tp1_end_ts = 1422806400\n",
    "tp1_test_p1_s1_features,tp1_test_p1_s1_labels = getFeaturesFromFile('test_data/'+file,tp1_start_ts,tp1_end_ts,tp1_window_size)\n",
    "save_object(tp1_test_p1_s1_features, \"q14_tp1_test_p1_s1_features\")\n",
    "save_object(tp1_test_p1_s1_labels, \"q14_tp1_test_p1_s1_labels\")\n",
    "print(len(tp1_test_p1_s1_features))\n",
    "# tp1_test_p1_s1_features = load_object(\"q14_tp1_test_p1_s1_features\")\n",
    "# tp1_test_p1_s1_labels = load_object(\"q14_tp1_test_p1_s1_labels\")\n",
    "print(\"Finished Building test feature vectors for time period 1 sample 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building test set for Timeperiod 1(sample 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Finished Building test feature vectors for time period 1 sample 2\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "file = 'sample2_period1.txt'\n",
    "\n",
    "#tp1\n",
    "tp1_window_size = 3600 # 1 hour window size\n",
    "tp1_start_ts = tp1_window_size * math.floor(filesToMinTs[file]/(tp1_window_size*1.0))\n",
    "tp1_end_ts = 1422806400\n",
    "tp1_test_p1_s2_features,tp1_test_p1_s2_labels = getFeaturesFromFile('test_data/'+file,tp1_start_ts,tp1_end_ts,tp1_window_size)\n",
    "save_object(tp1_test_p1_s2_features, \"q14_tp1_test_p1_s2_features\")\n",
    "save_object(tp1_test_p1_s2_labels, \"q14_tp1_test_p1_s2_labels\")\n",
    "print(len(tp1_test_p1_s2_features))\n",
    "# tp1_test_p1_s2_features = load_object(\"q14_tp1_test_p1_s2_features\")\n",
    "# tp1_test_p1_s2_labels = load_object(\"q14_tp1_test_p1_s2_labels\")\n",
    "print(\"Finished Building test feature vectors for time period 1 sample 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression Model for Time period 1\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.tools.eval_measures as ste\n",
    "\n",
    "print('\\nLinear Regression Model for Time period 1')\n",
    "X_orig = tp1_train_features\n",
    "y = tp1_train_labels\n",
    "\n",
    "# https://becominghuman.ai/stats-models-vs-sklearn-for-linear-regression-f19df95ad99b\n",
    "X = sm.add_constant(X_orig)\n",
    "\n",
    "model = sm.OLS(y,X)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0:\n",
      " MSE : 40197.85888844652\n",
      "[192.63069413 196.82015362 271.93418781 332.04939508 349.71110204\n",
      " 302.64963134]\n",
      "\n",
      "Sample 1:\n",
      " MSE : 125828.83613818546\n",
      "[172.6480249  179.28249985 217.94306262 332.11548282 529.50718169\n",
      " 773.80424573]\n",
      "\n",
      "Sample 2:\n",
      " MSE : 298897.98915656423\n",
      "[682.77077015 588.38237051 598.9167575  650.10339882 635.2280852\n",
      " 654.51393027]\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##sample 0\n",
    "pred_y = results.predict(tp1_test_p1_s0_features)\n",
    "y = tp1_test_p1_s0_labels\n",
    "print(\"\\nSample 0:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 1\n",
    "pred_y = results.predict(tp1_test_p1_s1_features)\n",
    "y = tp1_test_p1_s1_labels\n",
    "print(\"\\nSample 1:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 2\n",
    "pred_y = results.predict(tp1_test_p1_s2_features)\n",
    "y = tp1_test_p1_s2_labels\n",
    "print(\"\\nSample 2:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "print('---'*20)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random forest Model for Time period 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# {'max_depth': 60, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "# 'max_depth': 60, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 2000}\n",
    "\n",
    "print('\\nRandom forest Model for Time period 1')\n",
    "X = tp1_train_features\n",
    "y = tp1_train_labels\n",
    "model = GradientBoostingRegressor(max_depth=60,max_features='sqrt',min_samples_leaf=4,min_samples_split=10,n_estimators=2000)\n",
    "results = model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0:\n",
      " MSE : 231255.68839953945\n",
      "[ 180.08841007  275.62248463 1121.61081071  569.60182612  175.89926275\n",
      "  314.97616984]\n",
      "\n",
      "Sample 1:\n",
      " MSE : 144006.480175517\n",
      "[194.80398337 253.74421148 309.06175016 639.49496427 478.64147017\n",
      " 847.83472149]\n",
      "\n",
      "Sample 2:\n",
      " MSE : 24095.608009303236\n",
      "[477.56468142 162.11062202  78.50567379 134.49515699 198.16343835\n",
      "  60.42603916]\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##sample 0\n",
    "pred_y = results.predict(tp1_test_p1_s0_features)\n",
    "y = tp1_test_p1_s0_labels\n",
    "print(\"\\nSample 0:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 1\n",
    "pred_y = results.predict(tp1_test_p1_s1_features)\n",
    "y = tp1_test_p1_s1_labels\n",
    "print(\"\\nSample 1:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 2\n",
    "pred_y = results.predict(tp1_test_p1_s2_features)\n",
    "y = tp1_test_p1_s2_labels\n",
    "print(\"\\nSample 2:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "print('---'*20)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = tp1_train_features\n",
    "y = tp1_train_labels\n",
    "nn = MLPRegressor(hidden_layer_sizes=(100,100),max_iter=500)\n",
    "results = nn.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0:\n",
      " MSE : 219834.05763586448\n",
      "[272.80979269  60.14484789 522.29080646 582.34456939 338.67834234\n",
      " 918.18987969]\n",
      "\n",
      "Sample 1:\n",
      " MSE : 812685.1415464861\n",
      "[ 160.92315844  -22.40645831 -180.62544082  561.67383681  991.39088493\n",
      " 2139.84492575]\n",
      "\n",
      "Sample 2:\n",
      " MSE : 314074.3196384067\n",
      "[1498.9679989   242.35242446   52.6121603    67.90406497  -40.77353314\n",
      "   25.64220967]\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##sample 0\n",
    "pred_y = results.predict(tp1_test_p1_s0_features)\n",
    "y = tp1_test_p1_s0_labels\n",
    "print(\"\\nSample 0:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 1\n",
    "pred_y = results.predict(tp1_test_p1_s1_features)\n",
    "y = tp1_test_p1_s1_labels\n",
    "print(\"\\nSample 1:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 2\n",
    "pred_y = results.predict(tp1_test_p1_s2_features)\n",
    "y = tp1_test_p1_s2_labels\n",
    "print(\"\\nSample 2:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "print('---'*20)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeperiod 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building training set for Timeperiod 2(same as q7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Building feature vectors for training set for time period 2\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "#tp2\n",
    "tp2_window_size = 300 # 5 minute window size\n",
    "tp2_start_ts = 1422806400\n",
    "tp2_end_ts = 1422849600\n",
    "# tp2_train_features,tp2_train_labels = getFeatures(tp2_start_ts,tp2_end_ts,tp2_window_size)\n",
    "# save_object(tp2_train_features, \"q14_tp2_train_features\")\n",
    "# save_object(tp2_train_labels, \"q14_tp2_train_labels\")\n",
    "tp2_train_features = load_object(\"q14_tp2_train_features\")\n",
    "tp2_train_labels = load_object(\"q14_tp2_train_labels\")\n",
    "print(\"Finished Building feature vectors for training set for time period 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building test set for Timeperiod 2(sample 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Finished Building test feature vectors for time period 2 sample 0\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "file = 'sample0_period2.txt'\n",
    "\n",
    "#tp1\n",
    "tp2_window_size = 300 # 5 minute window size\n",
    "tp2_start_ts = 1422806400\n",
    "tp2_end_ts = 1422849600\n",
    "tp2_test_p2_s0_features,tp2_test_p2_s0_labels = getFeaturesFromFile('test_data/'+file,tp2_start_ts,tp2_end_ts,tp2_window_size)\n",
    "print(len(tp2_test_p2_s0_features))\n",
    "save_object(tp2_test_p2_s0_features, \"q14_tp2_test_p2_s0_features\")\n",
    "save_object(tp2_test_p2_s0_labels, \"q14_tp2_test_p2_s0_labels\")\n",
    "print(\"Finished Building test feature vectors for time period 2 sample 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building test set for Timeperiod 2(sample 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Finished Building test feature vectors for time period 2 sample 1\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "file = 'sample1_period2.txt'\n",
    "\n",
    "tp2_window_size = 300 # 5 minute window size\n",
    "tp2_start_ts = 1422806400\n",
    "tp2_end_ts = 1422849600\n",
    "tp2_test_p2_s1_features,tp2_test_p2_s1_labels = getFeaturesFromFile('test_data/'+file,tp2_start_ts,tp2_end_ts,tp2_window_size)\n",
    "print(len(tp2_test_p2_s1_features))\n",
    "save_object(tp2_test_p2_s1_features, \"q14_tp2_test_p2_s1_features\")\n",
    "save_object(tp2_test_p2_s1_labels, \"q14_tp2_test_p2_s1_labels\")\n",
    "print(\"Finished Building test feature vectors for time period 2 sample 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building test set for Timeperiod 2(sample 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Finished Building test feature vectors for time period 2 sample 2\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "file = 'sample2_period2.txt'\n",
    "\n",
    "\n",
    "tp2_window_size = 300 # 5 minute window size\n",
    "tp2_start_ts = 1422806400\n",
    "tp2_end_ts = 1422849600\n",
    "tp2_test_p2_s2_features,tp2_test_p2_s2_labels = getFeaturesFromFile('test_data/'+file,tp2_start_ts,tp2_end_ts,tp2_window_size)\n",
    "print(len(tp2_test_p2_s2_features))\n",
    "save_object(tp2_test_p2_s2_features, \"q14_tp2_test_p2_s2_features\")\n",
    "save_object(tp2_test_p2_s2_labels, \"q14_tp2_test_p2_s2_labels\")\n",
    "print(\"Finished Building test feature vectors for time period 2 sample 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression Model for Time period 2\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.tools.eval_measures as ste\n",
    "\n",
    "print('\\nLinear Regression Model for Time period 2')\n",
    "X_orig = tp2_train_features\n",
    "y = tp2_train_labels\n",
    "\n",
    "# https://becominghuman.ai/stats-models-vs-sklearn-for-linear-regression-f19df95ad99b\n",
    "X = sm.add_constant(X_orig)\n",
    "\n",
    "model = sm.OLS(y,X)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0:\n",
      " MSE : 3026058.6288454114\n",
      "[ 739.02639811 4519.08107638 4710.64660738 3038.95175338 2087.72055165\n",
      " 1964.02475419 1789.15749376]\n",
      "\n",
      "Sample 1:\n",
      " MSE : 526889.139481663\n",
      "[ 471.75038795 1372.46862476 1473.4414827  1318.29903241 1678.67058252\n",
      " 1284.38389114 1416.10150819]\n",
      "\n",
      "Sample 2:\n",
      " MSE : 115165.93501020844\n",
      "[334.09457888 328.32251906 341.18019657 360.8365362  345.05382627\n",
      " 436.30110523]\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##sample 0\n",
    "pred_y = results.predict(tp2_test_p2_s0_features)\n",
    "y = tp2_test_p2_s0_labels\n",
    "print(\"\\nSample 0:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 1\n",
    "pred_y = results.predict(tp2_test_p2_s1_features)\n",
    "y = tp2_test_p2_s1_labels\n",
    "print(\"\\nSample 1:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 2\n",
    "pred_y = results.predict(tp2_test_p2_s2_features)\n",
    "y = tp2_test_p2_s2_labels\n",
    "print(\"\\nSample 2:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "print('---'*20)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random forest Model for Time period 2\n"
     ]
    }
   ],
   "source": [
    "# {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# {'max_depth': 60, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "# {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200}\n",
    "# {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200}\n",
    "print('\\nRandom forest Model for Time period 2')\n",
    "X = tp2_train_features\n",
    "y = tp2_train_labels\n",
    "model = GradientBoostingRegressor(max_depth=20,max_features='sqrt',min_samples_leaf=4,min_samples_split=2,n_estimators=200)\n",
    "results = model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0:\n",
      " MSE : 4014921.5498217354\n",
      "[1161.90341733 1501.96529687 1732.65913814 2987.46086705 4659.30816717\n",
      " 2546.23430965  963.23523849]\n",
      "\n",
      "Sample 1:\n",
      " MSE : 1978235.2283417343\n",
      "[1161.90341733 1069.88520001 2462.75739337 3200.02013853  911.83124961\n",
      " 1069.88520001 2490.58831054]\n",
      "\n",
      "Sample 2:\n",
      " MSE : 2886768.425517559\n",
      "[1161.90341733 1161.90341733 1161.90341733 1161.90341733 1161.90341733\n",
      " 3296.15231321]\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##sample 0\n",
    "pred_y = results.predict(tp2_test_p2_s0_features)\n",
    "y = tp2_test_p2_s0_labels\n",
    "print(\"\\nSample 0:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 1\n",
    "pred_y = results.predict(tp2_test_p2_s1_features)\n",
    "y = tp2_test_p2_s1_labels\n",
    "print(\"\\nSample 1:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 2\n",
    "pred_y = results.predict(tp2_test_p2_s2_features)\n",
    "y = tp2_test_p2_s2_labels\n",
    "print(\"\\nSample 2:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "print('---'*20)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = tp2_train_features\n",
    "y = tp2_train_labels\n",
    "nn = MLPRegressor(hidden_layer_sizes=(100, 100, 100, 100, 100),max_iter=500)\n",
    "results = nn.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0:\n",
      " MSE : 3786581.888847169\n",
      "[5.32159002e-01 5.61460959e+02 3.86421987e+03 1.77988228e+03\n",
      " 5.41987015e+02 6.29308533e+02 5.43647912e+02]\n",
      "\n",
      "Sample 1:\n",
      " MSE : 1084366.5479518687\n",
      "[  24.30751355  222.57564731 -266.60479676  804.68047688 2761.84764243\n",
      "   86.44898596  752.12550257]\n",
      "\n",
      "Sample 2:\n",
      " MSE : 13757.488097773421\n",
      "[ -59.13686614   10.46342818  -16.59884067  235.60186686  145.82387177\n",
      " -133.26887018]\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##sample 0\n",
    "pred_y = results.predict(tp2_test_p2_s0_features)\n",
    "y = tp2_test_p2_s0_labels\n",
    "print(\"\\nSample 0:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 1\n",
    "pred_y = results.predict(tp2_test_p2_s1_features)\n",
    "y = tp2_test_p2_s1_labels\n",
    "print(\"\\nSample 1:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 2\n",
    "pred_y = results.predict(tp2_test_p2_s2_features)\n",
    "y = tp2_test_p2_s2_labels\n",
    "print(\"\\nSample 2:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "print('---'*20)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeperiod 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building training set for Timeperiod 3(same as q7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Building feature vectors for training set for time period 2\n"
     ]
    }
   ],
   "source": [
    "#tp3\n",
    "tp3_window_size = 3600 # 1 hour window size\n",
    "tp3_start_ts = 1422849600\n",
    "tp3_end_ts = tp3_window_size * math.ceil(globalMaxTs/(tp3_window_size*1.0))\n",
    "# tp3_train_features,tp3_train_labels = getFeatures(tp3_start_ts,tp3_end_ts,tp3_window_size)\n",
    "# save_object(tp3_train_features, \"q14_tp3_train_features\")\n",
    "# save_object(tp3_train_labels, \"q14_tp3_train_labels\")\n",
    "tp3_train_features = load_object(\"q14_tp3_train_features\")\n",
    "tp3_train_labels = load_object(\"q14_tp3_train_labels\")\n",
    "\n",
    "print(\"Finished Building feature vectors for training set for time period 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building test set for Timeperiod 3(sample 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Finished Building test feature vectors for time period 3 sample 0\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "file = 'sample0_period3.txt'\n",
    "\n",
    "tp3_window_size = 3600 # 1 hour window size\n",
    "tp3_start_ts = 1422849600\n",
    "tp3_end_ts = tp3_window_size * math.ceil(globalMaxTs/(tp3_window_size*1.0))\n",
    "tp3_test_p3_s0_features,tp3_test_p3_s0_labels = getFeaturesFromFile('test_data/'+file,tp3_start_ts,tp3_end_ts,tp3_window_size)\n",
    "print(len(tp3_test_p3_s0_features))\n",
    "save_object(tp3_test_p3_s0_features, \"q14_tp3_test_p3_s0_features\")\n",
    "save_object(tp3_test_p3_s0_labels, \"q14_tp3_test_p3_s0_labels\")\n",
    "print(\"Finished Building test feature vectors for time period 3 sample 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building test set for Timeperiod 3(sample 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Finished Building test feature vectors for time period 3 sample 1\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "file = 'sample1_period3.txt'\n",
    "\n",
    "tp3_window_size = 3600 # 1 hour window size\n",
    "tp3_start_ts = 1422849600\n",
    "tp3_end_ts = tp3_window_size * math.ceil(globalMaxTs/(tp3_window_size*1.0))\n",
    "tp3_test_p3_s1_features,tp3_test_p3_s1_labels = getFeaturesFromFile('test_data/'+file,tp3_start_ts,tp3_end_ts,tp3_window_size)\n",
    "print(len(tp3_test_p3_s1_features))\n",
    "save_object(tp3_test_p3_s1_features, \"q14_tp3_test_p3_s1_features\")\n",
    "save_object(tp3_test_p3_s1_labels, \"q14_tp3_test_p3_s1_labels\")\n",
    "print(\"Finished Building test feature vectors for time period 3 sample 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building test set for Timeperiod 3(sample 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Finished Building test feature vectors for time period 3 sample 2\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "file = 'sample2_period3.txt'\n",
    "\n",
    "tp3_window_size = 3600 # 1 hour window size\n",
    "tp3_start_ts = 1422849600\n",
    "tp3_end_ts = tp3_window_size * math.ceil(globalMaxTs/(tp3_window_size*1.0))\n",
    "tp3_test_p3_s2_features,tp3_test_p3_s2_labels = getFeaturesFromFile('test_data/'+file,tp3_start_ts,tp3_end_ts,tp3_window_size)\n",
    "print(len(tp3_test_p3_s2_features))\n",
    "save_object(tp3_test_p3_s2_features, \"q14_tp3_test_p3_s2_features\")\n",
    "save_object(tp3_test_p3_s2_labels, \"q14_tp3_test_p3_s2_labels\")\n",
    "print(\"Finished Building test feature vectors for time period 3 sample 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression Model for Time period 3\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.tools.eval_measures as ste\n",
    "\n",
    "print('\\nLinear Regression Model for Time period 3')\n",
    "X_orig = tp3_train_features\n",
    "y = tp3_train_labels\n",
    "\n",
    "# https://becominghuman.ai/stats-models-vs-sklearn-for-linear-regression-f19df95ad99b\n",
    "X = sm.add_constant(X_orig)\n",
    "\n",
    "model = sm.OLS(y,X)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0:\n",
      " MSE : 14853.867722326375\n",
      "[ 69.98283104  89.23745877 145.55940581 155.14803824 206.22973913\n",
      " 241.20994798]\n",
      "\n",
      "Sample 1:\n",
      " MSE : 251157.80755276975\n",
      "[713.62824577 777.67642828 778.95562111  13.89773527  56.60245611\n",
      "  89.88113324]\n",
      "\n",
      "Sample 2:\n",
      " MSE : 437919.34939268\n",
      "[633.10659979 678.57527832 678.95785623 713.62824577 777.67642828\n",
      " 778.95562111]\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##sample 0\n",
    "pred_y = results.predict(tp3_test_p3_s0_features)\n",
    "y = tp3_test_p3_s0_labels\n",
    "print(\"\\nSample 0:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 1\n",
    "pred_y = results.predict(tp3_test_p3_s1_features)\n",
    "y = tp3_test_p3_s1_labels\n",
    "print(\"\\nSample 1:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 2\n",
    "pred_y = results.predict(tp3_test_p3_s2_features)\n",
    "y = tp3_test_p3_s2_labels\n",
    "print(\"\\nSample 2:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "print('---'*20)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random forest Model for Time period 3\n"
     ]
    }
   ],
   "source": [
    "# {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# {'max_depth': 60, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "# {'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "\n",
    "print('\\nRandom forest Model for Time period 3')\n",
    "X = tp3_train_features\n",
    "y = tp3_train_labels\n",
    "model = GradientBoostingRegressor(max_depth=10,max_features='auto',min_samples_leaf=1,min_samples_split=10,n_estimators=200)\n",
    "results = model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0:\n",
      " MSE : 846.331871640485\n",
      "[67.64548512 64.54813869 52.84695803 54.84532447 65.10414108 52.84695803]\n",
      "\n",
      "Sample 1:\n",
      " MSE : 219792.98579223524\n",
      "[1234.04437618   53.14238311   37.90781635   22.96061632   54.84532447\n",
      "   48.00385414]\n",
      "\n",
      "Sample 2:\n",
      " MSE : 219671.5664696974\n",
      "[  66.14743631   52.32299923   65.6109342  1234.04437618   53.14238311\n",
      "   37.90781635]\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##sample 0\n",
    "pred_y = results.predict(tp3_test_p3_s0_features)\n",
    "y = tp3_test_p3_s0_labels\n",
    "print(\"\\nSample 0:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 1\n",
    "pred_y = results.predict(tp3_test_p3_s1_features)\n",
    "y = tp3_test_p3_s1_labels\n",
    "print(\"\\nSample 1:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 2\n",
    "pred_y = results.predict(tp3_test_p3_s2_features)\n",
    "y = tp3_test_p3_s2_labels\n",
    "print(\"\\nSample 2:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "print('---'*20)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = tp3_train_features\n",
    "y = tp3_train_labels\n",
    "nn = MLPRegressor(hidden_layer_sizes=(200,200),max_iter=500)\n",
    "results = nn.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0:\n",
      " MSE : 684099756.299843\n",
      "[-63424.51208286  -2041.40480846  -1397.65843932  -1769.20167756\n",
      "  -7692.13524975  -2296.54626633]\n",
      "\n",
      "Sample 1:\n",
      " MSE : 5757993790.952148\n",
      "[-182664.00795983  -33736.04092973    -645.94643409   -1027.44448818\n",
      "   -2294.23713599   -1389.48522874]\n",
      "\n",
      "Sample 2:\n",
      " MSE : 5795040167.005656\n",
      "[  -2214.53237908  -14645.66951      -3086.75441492 -182664.00795983\n",
      "  -33736.04092973    -645.94643409]\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##sample 0\n",
    "pred_y = results.predict(tp3_test_p3_s0_features)\n",
    "y = tp3_test_p3_s0_labels\n",
    "print(\"\\nSample 0:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 1\n",
    "pred_y = results.predict(tp3_test_p3_s1_features)\n",
    "y = tp3_test_p3_s1_labels\n",
    "print(\"\\nSample 1:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "\n",
    "##sample 2\n",
    "pred_y = results.predict(tp3_test_p3_s2_features)\n",
    "y = tp3_test_p3_s2_labels\n",
    "print(\"\\nSample 2:\\n MSE : {}\".format(ste.mse(pred_y, y,axis=0)))\n",
    "print(pred_y)\n",
    "print('---'*20)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
